{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4d73d07",
   "metadata": {},
   "source": [
    "# Implementing Q-Learning in Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c87fd1",
   "metadata": {},
   "source": [
    "- Implement a Q-Learning algorithm using Keras\n",
    "- Define and train a neural network to approximate the Q-values\n",
    "- Evaluate the performance of the trained Q-Learning agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbdfbd83",
   "metadata": {},
   "source": [
    "#### Step 1: Setting Up the Environment "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3ea9bb",
   "metadata": {},
   "source": [
    "We will set up the environment using the OpenAI Gym Library. We will use the 'CartPole-v1' environment, a common benchmark for reinforcement learning algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "fe3fcfeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym in c:\\users\\sekai\\.conda\\envs\\gpu\\lib\\site-packages (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.18.0 in c:\\users\\sekai\\.conda\\envs\\gpu\\lib\\site-packages (from gym) (1.23.5)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\sekai\\.conda\\envs\\gpu\\lib\\site-packages (from gym) (3.1.1)\n",
      "Requirement already satisfied: importlib_metadata>=4.8.0 in c:\\users\\sekai\\.conda\\envs\\gpu\\lib\\site-packages (from gym) (8.5.0)\n",
      "Requirement already satisfied: gym_notices>=0.0.4 in c:\\users\\sekai\\.conda\\envs\\gpu\\lib\\site-packages (from gym) (0.1.0)\n",
      "Requirement already satisfied: zipp>=3.20 in c:\\users\\sekai\\.conda\\envs\\gpu\\lib\\site-packages (from importlib_metadata>=4.8.0->gym) (3.21.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting gymnasium\n",
      "  Downloading gymnasium-1.1.1-py3-none-any.whl.metadata (9.4 kB)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\sekai\\.conda\\envs\\gpu\\lib\\site-packages (from gymnasium) (1.23.5)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\sekai\\.conda\\envs\\gpu\\lib\\site-packages (from gymnasium) (3.1.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.0 in c:\\users\\sekai\\.conda\\envs\\gpu\\lib\\site-packages (from gymnasium) (8.5.0)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\sekai\\.conda\\envs\\gpu\\lib\\site-packages (from gymnasium) (4.15.0)\n",
      "Collecting farama-notifications>=0.0.1 (from gymnasium)\n",
      "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl.metadata (558 bytes)\n",
      "Requirement already satisfied: zipp>=3.20 in c:\\users\\sekai\\.conda\\envs\\gpu\\lib\\site-packages (from importlib-metadata>=4.8.0->gymnasium) (3.21.0)\n",
      "Downloading gymnasium-1.1.1-py3-none-any.whl (965 kB)\n",
      "   ---------------------------------------- 0.0/965.4 kB ? eta -:--:--\n",
      "   -------------------------------- ------- 786.4/965.4 kB 4.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 965.4/965.4 kB 3.5 MB/s eta 0:00:00\n",
      "Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
      "Installing collected packages: farama-notifications, gymnasium\n",
      "\n",
      "   -------------------- ------------------- 1/2 [gymnasium]\n",
      "   -------------------- ------------------- 1/2 [gymnasium]\n",
      "   -------------------- ------------------- 1/2 [gymnasium]\n",
      "   -------------------- ------------------- 1/2 [gymnasium]\n",
      "   -------------------- ------------------- 1/2 [gymnasium]\n",
      "   -------------------- ------------------- 1/2 [gymnasium]\n",
      "   -------------------- ------------------- 1/2 [gymnasium]\n",
      "   -------------------- ------------------- 1/2 [gymnasium]\n",
      "   -------------------- ------------------- 1/2 [gymnasium]\n",
      "   ---------------------------------------- 2/2 [gymnasium]\n",
      "\n",
      "Successfully installed farama-notifications-0.0.4 gymnasium-1.1.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install gym\n",
    "%pip install gymnasium\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de3be1d",
   "metadata": {},
   "source": [
    "##### Reduce Recursion Limit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00bd5fbc",
   "metadata": {},
   "source": [
    "We can also try increasing the recursion limit, although this is generally more of a workaround than a solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "571fb9e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.setrecursionlimit(1500)\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "# Create the environment \n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "env.action_space.seed(42)\n",
    "env.observation_space.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b56956",
   "metadata": {},
   "source": [
    "- `gym` is a toolkit for developing and comparing reinforcement learning algorithms.\n",
    "- `CartPole-v1` is an environment where a pole is balanced on a cart, and the goal is to prevent the pole from falling over.\n",
    "- Setting random seeds ensures that we can reproduce the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c044b8e",
   "metadata": {},
   "source": [
    "#### Step 2: Define the Q-Learning Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808705ab",
   "metadata": {},
   "source": [
    "We will define a neural network using Keras to approximate the Q-values. The network will take the state as input and output Q-values for each action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "3b1e95d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n"
     ]
    }
   ],
   "source": [
    "# Suppress warnings for a cleaner notebook experience\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Override the default warning function\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "warnings.warn = warn\n",
    "\n",
    "# Import necessary libraries for the Q-Learning model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Define the model building function \n",
    "def build_model(state_size, action_size):\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(state_size))) # Use Input layer to specify the input shape\n",
    "    model.add(Dense(24, activation='relu'))\n",
    "    model.add(Dense(24, activation='relu'))\n",
    "    model.add(Dense(action_size, activation='linear'))\n",
    "    model.compile(loss='mse', optimizer=Adam(learning_rate=0.001))\n",
    "    return model\n",
    "\n",
    "# Create the environment and set up the model\n",
    "env = gym.make('CartPole-v1')\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "model = build_model(state_size, action_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613831b6",
   "metadata": {},
   "source": [
    "- Sequential model: a linear stack of layers in Keras\n",
    "- Dense layers: fully connected layers\n",
    "- input_dim: the size of the input layer, corresponding to the state size.\n",
    "- activation='relu': Rectified Linear Unit activation function.\n",
    "- activation='linear': linear activation function for the output layer, as we are predicting continuous Q-values.\n",
    "- Adam optimizer: an optimization algorithm that adjusts the learning rate based on gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c90bac6",
   "metadata": {},
   "source": [
    "#### Step 3: Implent the Q-Learning Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998e4120",
   "metadata": {},
   "source": [
    "Now, we will implement the Q-Learning algorithm, which involves interacting with the environment, updatin ghte Q-values, and training the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "39d52101",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1/100, score: 13, e: 1.0\n",
      "episode: 2/100, score: 15, e: 1.0\n",
      "episode: 3/100, score: 11, e: 1.0\n",
      "episode: 4/100, score: 14, e: 1.0\n",
      "episode: 5/100, score: 9, e: 1.0\n",
      "episode: 6/100, score: 40, e: 0.92\n",
      "episode: 7/100, score: 83, e: 0.78\n",
      "episode: 8/100, score: 88, e: 0.65\n",
      "episode: 9/100, score: 30, e: 0.61\n",
      "episode: 10/100, score: 36, e: 0.56\n",
      "episode: 11/100, score: 147, e: 0.42\n",
      "episode: 12/100, score: 143, e: 0.31\n",
      "episode: 17/100, score: 188, e: 0.043\n",
      "episode: 27/100, score: 184, e: 0.0099\n",
      "episode: 32/100, score: 117, e: 0.0099\n",
      "episode: 35/100, score: 101, e: 0.0099\n",
      "episode: 36/100, score: 111, e: 0.0099\n",
      "episode: 38/100, score: 148, e: 0.0099\n",
      "episode: 40/100, score: 86, e: 0.0099\n",
      "episode: 41/100, score: 91, e: 0.0099\n",
      "episode: 42/100, score: 128, e: 0.0099\n",
      "episode: 44/100, score: 181, e: 0.0099\n",
      "episode: 45/100, score: 138, e: 0.0099\n",
      "episode: 47/100, score: 94, e: 0.0099\n",
      "episode: 49/100, score: 93, e: 0.0099\n",
      "episode: 52/100, score: 175, e: 0.0099\n",
      "episode: 53/100, score: 91, e: 0.0099\n",
      "episode: 56/100, score: 133, e: 0.0099\n",
      "episode: 57/100, score: 93, e: 0.0099\n",
      "episode: 58/100, score: 82, e: 0.0099\n",
      "episode: 59/100, score: 164, e: 0.0099\n",
      "episode: 61/100, score: 105, e: 0.0099\n",
      "episode: 62/100, score: 176, e: 0.0099\n",
      "episode: 64/100, score: 100, e: 0.0099\n",
      "episode: 65/100, score: 128, e: 0.0099\n",
      "episode: 66/100, score: 81, e: 0.0099\n",
      "episode: 67/100, score: 107, e: 0.0099\n",
      "episode: 68/100, score: 139, e: 0.0099\n",
      "episode: 71/100, score: 121, e: 0.0099\n",
      "episode: 72/100, score: 127, e: 0.0099\n",
      "episode: 74/100, score: 176, e: 0.0099\n",
      "episode: 75/100, score: 159, e: 0.0099\n",
      "episode: 77/100, score: 161, e: 0.0099\n",
      "episode: 78/100, score: 68, e: 0.0099\n",
      "episode: 79/100, score: 89, e: 0.0099\n",
      "episode: 86/100, score: 124, e: 0.0099\n",
      "episode: 90/100, score: 174, e: 0.0099\n",
      "episode: 91/100, score: 138, e: 0.0099\n",
      "episode: 92/100, score: 128, e: 0.0099\n",
      "episode: 98/100, score: 90, e: 0.0099\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import tensorflow as tf\n",
    "\n",
    "# Define epsilon and epsilon_decay\n",
    "epsilon = 1.0 # Starting with a high exploration rate\n",
    "epsilon_min = 0.01 # Minimum exploration rate\n",
    "epsilon_decay = 0.99 # Faster decay rate for epsilon after each episode\n",
    "\n",
    "# Replay memory\n",
    "memory = deque(maxlen=2000)\n",
    "\n",
    "# our agent stores experiences like \"I was in situation X, took action Y, got reward Z, and ended up in situation W\"\n",
    "def remember(state, action, reward, next_state, done):\n",
    "    # Store the experience in memory.\n",
    "    memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "def replay(batch_size=128):\n",
    "    # Train the model using a random smaple of experiences from memory.\n",
    "    if len(memory) < batch_size:\n",
    "        return # Skip replay if there's not enough experience\n",
    "    \n",
    "    minibatch = random.sample(memory, batch_size) # Sample a random batch from memory\n",
    "\n",
    "    # Extract information for batch processing \n",
    "    states = np.vstack([x[0] for x in minibatch])\n",
    "    actions = np.array([x[1] for x in minibatch])\n",
    "    rewards = np.array([x[2] for x in minibatch])\n",
    "    next_states = np.vstack([x[3] for x in minibatch])\n",
    "    dones = np.array([x[4] for x in minibatch])\n",
    "\n",
    "    # Predict Q-values for the next states in batch\n",
    "    q_next = model.predict(next_states)\n",
    "    # Predict Q-values for the current states in batch\n",
    "    q_target = model.predict(states)\n",
    "\n",
    "    # Vectorized update of target values\n",
    "    for i in range(batch_size):\n",
    "        target = rewards[i]\n",
    "        if not dones[i]:\n",
    "            target += 0.95 * np.amax(q_next[i]) # Update Q valye with the discotuned future reward\n",
    "        q_target[i][actions[i]] = target  # Update only the taken action's Q value\n",
    "\n",
    "    # Train the model with the updates targets in batch\n",
    "    model.fit(states, q_target, epochs=1, verbose=0)\n",
    "     # Train in batch mode\n",
    "\n",
    "     # Reduce exploration rate (epsilon) after each training step\n",
    "    global epsilon \n",
    "    if epsilon > epsilon_min:\n",
    "        epsilon *= epsilon_decay\n",
    "\n",
    "# High epsilon = mostly random actions (exploration)\n",
    "# Low epsilon = mostly \"smart\" actions based on what it learned    \n",
    "def act(state):\n",
    "    # Choose an action based on the current state and exploration rate.\n",
    "    if np.random.rand() <= epsilon:\n",
    "        return random.randrange(action_size) # Explore: choose a random action\n",
    "    act_values = model.predict(state) # Exploit: predict action based on the state\n",
    "    return np.argmax(act_values[0]) # Return the action with the highets Q-value\n",
    "\n",
    "# Define the number of episodes we want to train the model for\n",
    "episodes = 100 # We can set this to any number we prefer\n",
    "train_frequency = 5 # Train the model every 5 steps\n",
    "\n",
    "for e in range(episodes):\n",
    "    state, _ = env.reset() # Unpack the tuple returned by env.reset()\n",
    "    state = np.reshape(state, [1, state_size])\n",
    "    for time in range(200): # Limit to 200 time steps per episode\n",
    "        action = act(state)\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        reward = reward if not done else -10\n",
    "        next_state = np.reshape(next_state, [1, state_size])\n",
    "        remember(state, action, reward, next_state, done) # Store experience\n",
    "        state = next_state\n",
    "\n",
    "        if done:\n",
    "            print(f\"episode: {e+1}/{episodes}, score: {time}, e: {epsilon:.2}\")\n",
    "            break\n",
    "\n",
    "        # Train the model every 'train_frequency' steps \n",
    "        if time % train_frequency == 0:\n",
    "            replay(batch_size=64) # Call replay with larger batch size for efficiency\n",
    "\n",
    "env.close()\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ec07d4",
   "metadata": {},
   "source": [
    "The scores here are very volatile because epsilon forces bad random actions for learning purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156f1b28",
   "metadata": {},
   "source": [
    "##### Setup & Hyperparameters\n",
    "\n",
    "    epsilon = 1.0          # Start with 100% random actions\n",
    "    epsilon_min = 0.01     # Never go below 1% randomness  \n",
    "    epsilon_decay = 0.99   # Reduce epsilon by 1% each training step\n",
    "    memory = deque(maxlen=2000)  # Store last 2000 experiences\n",
    "This sets up our exploration strategy and experience storage.\n",
    "\n",
    "##### Experience Storage\n",
    "    def remember(state, action, reward, next_state, done):\n",
    "        memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "Every time our agent does something, it saves \"I was in state X, did action Y, got reward Z, ended up in state W, and the episode ended (True/False).\"\n",
    "\n",
    "##### Learning Function \n",
    "    def replay(batch_size=128):\n",
    "\n",
    "This is where the magic happens:\n",
    "1. Sample Random Experiences: Takes 128 random memories to learn from \n",
    "2. Predict the Future Values:\n",
    "\n",
    "    - q_next = model.predict(next_states) -> \"How good are actions in the next state?\"\n",
    "    - q_target = mode.predict(states) -> \"How good did we think actions were?\"\n",
    "3. Update Targets: For each experience:\n",
    "\n",
    "    `target = rewards[i]  # Immediate reward`\n",
    "    \n",
    "    `if not dones[i]:`\n",
    "\n",
    "        target += 0.95 * np.amax(q_next[i])  # + discounted future reward\n",
    "\n",
    "This implements the Bellman equation: Q(state, action) = reward + 0.95 Ã— max future reward\n",
    "\n",
    "4. Train the Network: Updates the neural network to predict these better target values.\n",
    "\n",
    "##### Action Selection\n",
    "    def act(state):\n",
    "        if np.random.rand() <= epsilon:\n",
    "            return random.randrange(action_size)  # Explore\n",
    "        act_values = model.predict(state)         # Get Q-values\n",
    "        return np.argmax(act_values[0])          # Pick best action\n",
    "\n",
    "Epsilon-greedy: flip a weighted coin to decide explore vs. exploit.\n",
    "\n",
    "##### Main Training Loop \n",
    "    for e in range(episodes):\n",
    "        state, _ = env.reset()  # Start new episode\n",
    "        for time in range(200):  # Max 200 steps\n",
    "            action = act(state)  # Choose action\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            \n",
    "            reward = reward if not done else -10  # Penalty for failing\n",
    "            remember(...)  # Store experience\n",
    "            \n",
    "            if time % train_frequency == 0:\n",
    "                replay(batch_size=64)  # Learn every 5 steps\n",
    "\n",
    "Key Insight: Our agent is constantly doign two things: \n",
    "    1. Acting in the environment (playing the same)\n",
    "    2. Learning from past experiences (getting better at the game)\n",
    "\n",
    "The brilliant part is it learsn from random past experiences, not just recent ones, which prevents it from forgetting old lessons."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dbcd6d6",
   "metadata": {},
   "source": [
    "#### Step 4: Evaluate the Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37b2a96",
   "metadata": {},
   "source": [
    "Finally, we will evaluate the performenace of the trained Q-Learning agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "9d7b3b47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1/10, score: 445\n",
      "episode: 2/10, score: 429\n",
      "episode: 3/10, score: 104\n",
      "episode: 4/10, score: 98\n",
      "episode: 5/10, score: 396\n",
      "episode: 6/10, score: 499\n",
      "episode: 7/10, score: 447\n",
      "episode: 8/10, score: 83\n",
      "episode: 9/10, score: 89\n",
      "episode: 10/10, score: 499\n"
     ]
    }
   ],
   "source": [
    "for e in range(10):\n",
    "    state, _ = env.reset() # Unpack the state from the tuple\n",
    "    state = np.reshape(state, [1, state_size]) # Reshape the state correectly\n",
    "    for time in range(500):\n",
    "        env.render()\n",
    "        action = np.argmax(model.predict(state)[0])\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action) # Unpack the five return values\n",
    "        done = terminated or truncated # Check if the episode is done\n",
    "        next_state = np.reshape(next_state, [1, state_size])\n",
    "        state = next_state\n",
    "        if done:\n",
    "            print(f\"episode: {e+1}/10, score: {time}\")\n",
    "            break\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f550603",
   "metadata": {},
   "source": [
    "Nearly perfect scores. However, the bimodal distribution looks concerning, but it is common in CartPole. Our agents has learned a solid policy but: \n",
    "- Sometimes it starts in a tricky state or makes one bad early decision\n",
    "- Once it recovers balance, it can maintain it almost indefinitely\n",
    "- The 499s hit the episode time limit, our agent could probably go forever."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
