{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2a378c1",
   "metadata": {},
   "source": [
    "# Transformers for Text Generation \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5016dcdb",
   "metadata": {},
   "source": [
    "- Implement Transformers for text generation tasks\n",
    "- Build, train, and evaluate Transformer models for text generation using TensorFlow and Keras\n",
    "- Apply text generation in real-world scenarios"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbca1e36",
   "metadata": {},
   "source": [
    "#### Step 1: Set up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c07882b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "# from tensorflow.keras.layers import TextVectorization\n",
    "try:\n",
    "    from tensorflow.keras.layers import TextVectorization\n",
    "except ImportError:\n",
    "    from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "\n",
    "from tensorflow.keras.utils import get_file\n",
    "\n",
    "# Load the dataset\n",
    "path_to_file = get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt') \n",
    "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
    "\n",
    "# Preview the dataset\n",
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5fd56f4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorized text shape:  (202646,)\n",
      "First 10 vectorized tokens:  [ 89 270 138  36 982 144 673 125  16 106]\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the data\n",
    "vocab_size = 10000 # Above 10000 for a dataset like this may result to overfitting and waste of memory.\n",
    "seq_length = 100\n",
    "\n",
    "# Adapt TextVectorization to full text\n",
    "vectorizer = TextVectorization(max_tokens=vocab_size, output_mode='int') # Create a TextVectorization layer that will convert text â†’ integer sequences\n",
    "text_ds = tf.data.Dataset.from_tensor_slices([text]).batch(1) # Put the whole text into a dataset (needed for the vectorizer to learn the words)\n",
    "vectorizer.adapt(text_ds) # Teach the vectorizer what words exist in the text and how often they appear\n",
    "\n",
    "\n",
    "# Vectorize the text\n",
    "vectorized_text = vectorizer([text])[0] # Turn the text into a list of numbers (one number per word)\n",
    "print(\"Vectorized text shape: \", vectorized_text.shape) # Show how many numbers (words) are in the text\n",
    "print(\"First 10 vectorized tokens: \", vectorized_text.numpy()[:10]) # Show the first 10 numbers as an example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531390d8",
   "metadata": {},
   "source": [
    "The Shakespeare dataset has 202,646 word tokens in total, and the model learns from only 10,000 out of the 202,646"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453c5f8c",
   "metadata": {},
   "source": [
    "In the code above: \n",
    "\n",
    "1. Built a vocabulary (up to 10,000 unique words).\n",
    " - The vectorizer scans the whole Shakespeare text and figures out the most common words. \n",
    " - Each word gets an ID number (e.g., \"the\" â†’ 5, \"love\" â†’ 28) (the numbers are assigned on how common these words appear, the more common the lower. [Also starts from 2, as 0 and 1 are reserved for padding and out of vocabulary tokens])\n",
    "\n",
    "2. Convert text â†’ integers.\n",
    " - The raw words in the play are replaced with their IDs.\n",
    " - Instead of: \"To be or not to be\" You get something like: [89, 270, 138, 36, 982, 144]\n",
    "\n",
    "3. Prepare for the model.\n",
    " - Neural networks canâ€™t process raw text.\n",
    " - By turning words into numbers, we can feed them into an embedding layer (which turns each word ID into a vector of floats) â†’ then into a Transformer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39162500",
   "metadata": {},
   "source": [
    "#### Step 2: Create input and target sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff1776c",
   "metadata": {},
   "source": [
    "Generate input and target sequences for training the Transformer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f01ee04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sequences generated:  202546\n",
      "Sample input sequence:  [  89  270  138   36  982  144  673  125   16  106   34  106  106   89\n",
      "  270    7   41   34 1286  344    4  200   64    4 3690   34 1286 1286\n",
      "   89  270   89    7   93 1187  225   12 2442  592    4    2  307   34\n",
      "   36 2655   36 2655   89  270   72   79  506   27    3   56   24 1390\n",
      "   57   40  161 2328  644    9 4980   34   32   54 2863  885   72   17\n",
      "   18  163  146  146  165  270   74  218   46  595   89  270   36   41\n",
      " 6739  172  595    2 1780   46   29 1323 5151   47   58 4151   79   39\n",
      "   60   58]\n",
      "Shape of X:  (202546, 100)\n",
      "Shape of Y:  (202546, 100)\n"
     ]
    }
   ],
   "source": [
    "def create_sequences(text, seq_length):\n",
    "    # These will store many training examples\n",
    "    input_seqs = [] # inputs the model sees\n",
    "    target_seqs =[] # correct answers the model should predict\n",
    "    \n",
    "    # Slide a window of length `seq_length` across the text\n",
    "    for i in range(len(text) - seq_length):\n",
    "        # Input = current window of tokens\n",
    "        input_seq = text[i:i + seq_length]\n",
    "        # Target = same window but shifted by one token\n",
    "        # This makes the \"next word prediction\" task\n",
    "        target_seq = text[i + 1:i + seq_length + 1]\n",
    "        # Save them\n",
    "        input_seqs.append(input_seq)\n",
    "        target_seqs.append(target_seq)\n",
    "    # Convert lists into arrays (for TensorFlow to use)\n",
    "    return np.array(input_seqs), np.array(target_seqs)\n",
    "\n",
    "# Generate sequences\n",
    "# Create input/output pairs from the vectorized Shakespeare text\n",
    "X, Y = create_sequences(vectorized_text.numpy(), seq_length)\n",
    "\n",
    "# Check if sequences are correctly generated \n",
    "\n",
    "# Show how many training pairs we built\n",
    "print(\"Number of sequences generated: \", len(X))\n",
    "# Show the first input sequence (just for inspection)\n",
    "print(\"Sample input sequence: \", X[0] if len(X) > 0 else \"No sequences generated\")\n",
    "\n",
    "# Check if X and Y are not empty\n",
    "# Make sure nothing went wrong (no empty data)\n",
    "assert X.size > 0, \"Input data X is empty\"\n",
    "assert Y.size > 0, \"Target data Y is empty\"\n",
    "\n",
    "# Convert arrays into TensorFlow tensors (so they can go into a model)\n",
    "X = tf.convert_to_tensor(X)\n",
    "Y = tf.convert_to_tensor(Y)\n",
    "\n",
    "# Print the final shapes\n",
    "print(\"Shape of X: \", X.shape)\n",
    "print(\"Shape of Y: \", Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132cc5fb",
   "metadata": {},
   "source": [
    "1. The Goal\n",
    "    We want the model to learn how to predict the next word in a sequence of text.\n",
    "To do that, we need to turn our long vector of words (202,646 tokens) into training examples.\n",
    "\n",
    "2. The Sliding Window\n",
    "    The function create_sequences(text, seq_length) creates a sliding window of length seq_length (say 100) across the text.\n",
    "\n",
    "    For each position i:\n",
    "    \n",
    "        Input sequence (X) = tokens from i to i + seq_length\n",
    "        Target sequence (Y) = the same tokens but shifted one step to the right\n",
    "\n",
    "    So the model is asked:\n",
    "    ðŸ‘‰ â€œIf you see this chunk of text, what comes next?â€\n",
    "\n",
    "3. Example (small numbers)\n",
    "    If text = [10, 20, 30, 40, 50] and seq_length = 3:\n",
    "        Step 0:\n",
    "            Input = [10, 20, 30]\n",
    "            Target = [20, 30, 40]\n",
    "\n",
    "        Step 1:\n",
    "            Input = [20, 30, 40]\n",
    "            Target = [30, 40, 50]\n",
    "\n",
    "        So we get:\n",
    "            X = [[10,20,30], [20,30,40]]\n",
    "            Y = [[20,30,40], [30,40,50]]\n",
    "\n",
    "4. Why Do This?\n",
    "    - The model doesnâ€™t just learn single-word predictions.\n",
    "\n",
    "    - It learns from overlapping contexts â€” every word appears many times in different positions.\n",
    "\n",
    "    - By training on millions of these pairs, the model picks up grammar, style, and sequence patterns.\n",
    "\n",
    "âœ… In plain English:\n",
    "This code chops the text into many overlapping â€œquestions and answers.â€\n",
    "\n",
    "    - The question (X) = â€œHere are 100 words in order.â€\n",
    "    - The answer (Y) = â€œHere are the next 100 words (shifted by one).â€\n",
    "    The model learns this mapping so it can predict the next word when generating text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648edbd6",
   "metadata": {},
   "source": [
    "Decode the IDs into words, so we can see what actaully is happening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4731be90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded sequence:\n",
      " first citizen before we proceed any further hear me speak all speak speak first citizen you are all resolved rather to die than to famish all resolved resolved first citizen first you know caius marcius is chief enemy to the people all we knowt we knowt first citizen let us kill him and well have corn at our own price ist a verdict all no more talking ont let it be done away away second citizen one word good citizens first citizen we are accounted poor citizens the patricians good what authority surfeits on would relieve us if they would\n"
     ]
    }
   ],
   "source": [
    "# Get the vocabulary list from the vectorizer\n",
    "# index -> word mapping\n",
    "vocab = vectorizer.get_vocabulary()\n",
    "\n",
    "# Sample input sequence (the numbers you printed)\n",
    "sample_sequence = X[0].numpy()\n",
    "\n",
    "# Decode numbers back into words\n",
    "decoded_words = [vocab[token_id] for token_id in sample_sequence]\n",
    "\n",
    "# Join into a readable string\n",
    "decoded_text = \" \".join(decoded_words)\n",
    "\n",
    "print(\"Decoded sequence:\\n\", decoded_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187d02a0",
   "metadata": {},
   "source": [
    "#### Step 3: Build the Transformer model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9cb557",
   "metadata": {},
   "source": [
    "Define the Transformer model architecture for text generation. \n",
    "\n",
    "In the following code:\n",
    " - Define the TransformerBlcok class that includes multi-head attention and feedforward layers with normalization and dropout.\n",
    " - Define the TransformerModel class, including embedding, positional encoding, and multiple Transformer blocks.\n",
    " - Compile the Transformer model using Adam optimizer and sparse categorical cross-entropy loss function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b184f810",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding, MultiHeadAttention, Dense, LayerNormalization, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "class TransformerBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        # Multi-head attention: \"what should I focus on?\"\n",
    "        self.att = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        # Feed-forward network: expand -> process -> compress\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            # Dense(ff_dim, activation=\"relu\"), # Expand and activate\n",
    "            Dense(ff_dim, activation=\"leaky_relu\"), # Expand and activate\n",
    "            Dense(embed_dim) # Compress back to original\n",
    "        ])\n",
    "        # Layer normalization: keeps values stable during training\n",
    "        self.layernorm1 = LayerNormalization(epsilon = 1e-6)\n",
    "        self.layernorm2 = LayerNormalization(epsilon = 1e-6)\n",
    "        # Dropout: randomly zero some values to prevent overfitting\n",
    "        self.dropout1 = Dropout(rate)\n",
    "        self.dropout2 = Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        # Step 1: Self-attention - each word looks at all other words\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output) # Add & normalize (residual connection)\n",
    "        # Step 2: Feed-forward processing\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output) # Add & normalize again\n",
    "    \n",
    "class TransformerModel(Model): \n",
    "    def __init__(self, vocab_size, embed_dim, num_heads, ff_dim, num_layers, seq_length):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        # Convert word IDs to dense vectors\n",
    "        self.embedding = Embedding(vocab_size, embed_dim)\n",
    "        # Add position information (since attention ignores word order)\n",
    "        self.pos_encoding = self.positional_encoding(seq_length, embed_dim)\n",
    "        # Stack multiple transformer blocks for deeper processing\n",
    "        self.transformer_blocks = [TransformerBlock(embed_dim, num_heads, ff_dim) for _ in range (num_layers)]\n",
    "        # Final layer: convert back to vocabulary predictions\n",
    "        self.dense = Dense(vocab_size)\n",
    "\n",
    "    def positional_encoding(self, seq_length, embed_dim):\n",
    "        # Calculate angles for sine/cosine functions\n",
    "        angle_rads = self.get_angles(np.arange(seq_length)[:, np.newaxis], np.arange(embed_dim)[np.newaxis, :], embed_dim)\n",
    "        # Apply sine to even indices, cosine to odd indices\n",
    "        angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2]) # Even positions\n",
    "        angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2]) # Odd positions\n",
    "        # Add batch dimension and convert to TensorFlow tensor\n",
    "        pos_encoding = angle_rads[np.newaxis, ...]\n",
    "        return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "    def get_angles(self, pos, i, embed_dim):\n",
    "        # Different frequencies for different dimensions\n",
    "        angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(embed_dim))\n",
    "        return pos * angle_rates\n",
    "    \n",
    "    def call(self, inputs, training=False):\n",
    "        seq_len = tf.shape(inputs)[1] # Get sequence length\n",
    "        # Step 1: Convert word IDs to embeddings\n",
    "        x = self.embedding(inputs)\n",
    "        # Step 2: Add positional information\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "        # Step 3: Pass through each transformer block\n",
    "        for transformer_block in self.transformer_blocks:\n",
    "            x = transformer_block(x, training=training)\n",
    "        # Step 4: Final prediction layer\n",
    "        output = self.dense(x)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9565a3f9",
   "metadata": {},
   "source": [
    "In the code above: (explanantion by Claude) \n",
    "\n",
    "The Big Picture\n",
    "\n",
    "This is building a Transformer model - the same architecture that powers ChatGPT, BERT, and most modern AI language models. Think of it as a really sophisticated pattern-matching machine that can understand relationships between words in a sentence.\n",
    "\n",
    "Part 1: TransformerBlock - The Core Processing Unit\n",
    "\n",
    "`class TransformerBlock(tf.keras.layers.Layer):`\n",
    "\n",
    "This is like one \"layer\" of thinking. The full model will stack multiple of these together. Each block does two main things:\n",
    "1. Multi-Head Attention - \"What should I pay attention to?\"\n",
    "\n",
    "`self.att = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "attn_output = self.att(inputs, inputs)`\n",
    "\n",
    "This is the magic sauce! It looks at every word in the sentence and asks: \"Given this word, which other words in the sentence are most important right now?\"\n",
    "For example, in \"The cat that was sleeping soundly woke up\", when processing \"woke\", the attention mechanism figures out that \"cat\" is super important (not \"sleeping\" or \"soundly\").\n",
    "\n",
    "2. Feed-Forward Network - \"Now let me think about what I learned\"\n",
    "\n",
    "`self.ffn = tf.keras.Sequential([\n",
    "    Dense(ff_dim, activation=\"relu\"),  # Expand and process\n",
    "    Dense(embed_dim)                   # Compress back down\n",
    "])`\n",
    "\n",
    "After figuring out what to pay attention to, it does some deeper processing - like \"OK, I know 'cat' and 'woke' are connected, now what does that actually mean?\"\n",
    "\n",
    "3. The Skip Connections and Normalization\n",
    "\n",
    "`out1 = self.layernorm1(inputs + attn_output)  # Add original input back\n",
    "return self.layernorm2(out1 + ffn_output)     # Add again`\n",
    "\n",
    "This is brilliant engineering - it keeps adding the original input back in. Why? Because it prevents the \"telephone game\" effect where information gets garbled as it passes through many layers.\n",
    "\n",
    "Part 2: TransformerModel - The Full Architecture\n",
    "\n",
    "Word Embeddings\n",
    "\n",
    "`self.embedding = Embedding(vocab_size, embed_dim)`\n",
    "\n",
    "Converts words to numbers. \"cat\" might become [0.2, -0.5, 0.8, ...] - a vector that captures the \"essence\" of what \"cat\" means.\n",
    "\n",
    "Positional Encoding - \"Where am I in the sentence?\"\n",
    "\n",
    "`self.pos_encoding = self.positional_encoding(seq_length, embed_dim)`\n",
    "\n",
    "Here's the weird part: the attention mechanism has no concept of word order! \"Cat dog\" and \"dog cat\" look identical to it. So we need to manually inject position information.\n",
    "The positional encoding uses sine and cosine waves to create a unique \"fingerprint\" for each position:\n",
    "\n",
    "Position 0: [sin(0), cos(0), sin(0), cos(0), ...]\n",
    "Position 1: [sin(1), cos(1), sin(1/100), cos(1/100), ...]\n",
    "Position 2: [sin(2), cos(2), sin(2/100), cos(2/100), ...]\n",
    "\n",
    "Why sine/cosine? Because they create smooth, predictable patterns that help the model understand \"this word comes before that word.\"\n",
    "The Processing Pipeline\n",
    "\n",
    "`def call(self, inputs, training=False):\n",
    "    x = self.embedding(inputs)           # Words â†’ numbers\n",
    "    x += self.pos_encoding[:, :seq_len, :] # Add position info\n",
    "    for transformer_block in self.transformer_blocks:\n",
    "        x = transformer_block(x, training=training)  # Stack multiple layers\n",
    "    output = self.dense(x)               # Final prediction`\n",
    "    \n",
    "What This Model Actually Does\n",
    "\n",
    "Takes in: A sequence of word IDs like [45, 123, 67, 891] (representing \"The cat is sleeping\")\n",
    "Converts: Each ID to a rich vector representation\n",
    "Adds: Position information so it knows word order\n",
    "Processes: Through multiple transformer blocks, each one getting better at understanding context\n",
    "Outputs: Predictions for what the next word should be\n",
    "\n",
    "Why This Architecture is Genius\n",
    "\n",
    "Parallel Processing: Unlike older models that had to process words one-by-one, this can look at all words simultaneously\n",
    "Long-Range Dependencies: It can connect \"The cat\" with \"woke up\" even with lots of words in between\n",
    "Scalable: You can make it smarter by adding more layers or making the embeddings bigger\n",
    "\n",
    "The whole thing is essentially a very sophisticated autocomplete system that's learned incredibly nuanced patterns about how language works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5d92ec9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"transformer_model_16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_16 (Embedding)     multiple                  2560000   \n",
      "_________________________________________________________________\n",
      "transformer_block_66 (Transf multiple                  1315840   \n",
      "_________________________________________________________________\n",
      "transformer_block_67 (Transf multiple                  1315840   \n",
      "_________________________________________________________________\n",
      "transformer_block_68 (Transf multiple                  1315840   \n",
      "_________________________________________________________________\n",
      "transformer_block_69 (Transf multiple                  1315840   \n",
      "_________________________________________________________________\n",
      "dense_156 (Dense)            multiple                  2570000   \n",
      "=================================================================\n",
      "Total params: 10,393,360\n",
      "Trainable params: 10,393,360\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "embed_dim = 256\n",
    "num_heads = 4\n",
    "ff_dim = 512\n",
    "num_layers = 4\n",
    "\n",
    "\n",
    "# Build the Transformer model\n",
    "model = TransformerModel(vocab_size, embed_dim, num_heads, ff_dim, num_layers, seq_length)\n",
    "\n",
    "# Provide input shape to build the model by passing a dummy input with maxval specified \n",
    "_ = model(tf.random.uniform((1, seq_length), maxval=vocab_size, dtype=tf.int32))\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.00002)\n",
    "# Compile the model \n",
    "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy')\n",
    "\n",
    "# Summary of the model\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4412e1cf",
   "metadata": {},
   "source": [
    "#### Step 4: Train the Transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b4f1f57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Available: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "Num GPUs Available:  1\n",
      "TensorFlow built with CUDA: True\n"
     ]
    }
   ],
   "source": [
    "print(\"GPU Available:\", tf.config.list_physical_devices('GPU'))\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "print(\"TensorFlow built with CUDA:\", tf.test.is_built_with_cuda())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33bd796b",
   "metadata": {},
   "source": [
    "Loss was increasing, had to lower the learning rate..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "46bf12b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "792/792 [==============================] - 302s 376ms/step - loss: 9.4701\n",
      "Epoch 2/20\n",
      "792/792 [==============================] - 299s 377ms/step - loss: 8.7744\n",
      "Epoch 3/20\n",
      "792/792 [==============================] - 298s 377ms/step - loss: 8.6641\n",
      "Epoch 4/20\n",
      "792/792 [==============================] - 298s 377ms/step - loss: 8.6103\n",
      "Epoch 5/20\n",
      "792/792 [==============================] - 298s 377ms/step - loss: 8.5987\n",
      "Epoch 6/20\n",
      "792/792 [==============================] - 298s 376ms/step - loss: 8.5754\n",
      "Epoch 7/20\n",
      "792/792 [==============================] - 297s 375ms/step - loss: 8.5485\n",
      "Epoch 8/20\n",
      "792/792 [==============================] - 297s 375ms/step - loss: 8.5481\n",
      "Epoch 9/20\n",
      "792/792 [==============================] - 297s 375ms/step - loss: 8.5313\n",
      "Epoch 10/20\n",
      "792/792 [==============================] - 297s 374ms/step - loss: 8.5323\n",
      "Epoch 11/20\n",
      "792/792 [==============================] - 297s 374ms/step - loss: 8.5171\n",
      "Epoch 12/20\n",
      "792/792 [==============================] - 297s 374ms/step - loss: 8.5103\n",
      "Epoch 13/20\n",
      "792/792 [==============================] - 297s 374ms/step - loss: 8.4970\n",
      "Epoch 14/20\n",
      "792/792 [==============================] - 297s 374ms/step - loss: 8.5296\n",
      "Epoch 15/20\n",
      "792/792 [==============================] - 297s 374ms/step - loss: 8.5027\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Early stopping callback to stop training if the loss doesn't improve\n",
    "early_stopping = EarlyStopping(monitor='loss', patience=2, restore_best_weights=True)\n",
    "\n",
    "# Train the transformer model on the full input and target sequences\n",
    "history = model.fit(X, Y, epochs=20, batch_size=256, callbacks=[early_stopping])\n",
    "\n",
    "# Plot training loss to monitor model performance over epochs\n",
    "plt.plot(history.history['loss'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3ea5c9",
   "metadata": {},
   "source": [
    "We ended up with a pretty bad output. Let's see what kind of text it can generate. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3d9960",
   "metadata": {},
   "source": [
    "#### Step 5: Generate text with the trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262f9573",
   "metadata": {},
   "source": [
    "Define a function to generate text using the trained Transformer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "03ef0c5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To be, or not to be siege duteous paly messenger midday whos froth deign likewise tempting requires kin soft lamberts happiness hard watches fancy fight heartily overmerry mounting bring thread giant shepherds bias corse heir deceiving sake nowwill forbear mocker tempter vowed usage homebred cakes ado distress forswear creep whatsoeer pink old drift burgundy lonely vented widowd music cowardice liege feeble scraping daughter commodity judgment bud firmly leanness remained broke remainder nosegays yearnd loathed exceeds pleased wounded townarmory draws merciless condemned treasure bawd rebel young rebate soils lions bianca for mellowd alike mans french mightiest singularities off palpable shakest pence oaths governd rotten tumble mystery remembers\n"
     ]
    }
   ],
   "source": [
    "def generate_text(model, start_string, num_generate=100, temperature=1.0):\n",
    "    # Convert the start string to a vectorized format. \n",
    "    input_eval = vectorizer([start_string]).numpy()\n",
    "\n",
    "    # Ensure the input length is the same as the model's expected input shape.\n",
    "    if input_eval.shape[1] < seq_length:\n",
    "        # Pad the input if it's shorter than the expected sequence length\n",
    "        padding = np.zeros((1, seq_length - input_eval.shape[1]))\n",
    "        input_eval = np.concatenate((padding, input_eval), axis=1)\n",
    "    elif input_eval.shape[1] > seq_length:\n",
    "        # Shorten the input if it's longer than the expected sequence length\n",
    "        input_eval = input_eval[:, -seq_length:]\n",
    "\n",
    "    input_eval = tf.convert_to_tensor(input_eval)\n",
    "\n",
    "    # Initialize an empty list to store generated text\n",
    "    text_generated = []\n",
    "\n",
    "    # Start generating text\n",
    "    for i in range(num_generate):\n",
    "        # Make predictions using the model\n",
    "        predictions = model(input_eval)\n",
    "\n",
    "        # Remove only the batch dimension, keep the logits as 2D (batch_size, vocab_size)\n",
    "        predictions = predictions[0] # This should be of shape [vocab_size]\n",
    "\n",
    "        # Apply temperature to predictions \n",
    "        predictions = predictions / temperature\n",
    "\n",
    "        # Use a categorical distribution to predict the next word\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[0, 0].numpy()\n",
    "\n",
    "        # Update the input tensor to include the predicted word, maintaining the sequence length\n",
    "        input_eval = np.append(input_eval.numpy(), [[predicted_id]], axis=1) # Append predicted token\n",
    "        input_eval = input_eval[:, -seq_length: ] # Keep only the last `seq_length` tokens\n",
    "        input_eval = tf.convert_to_tensor(input_eval) # Convert back to tensor\n",
    "\n",
    "        # Append the predicted word to the generated text\n",
    "        text_generated.append(vectorizer.get_vocabulary()[predicted_id])\n",
    "\n",
    "    # Return the generated text starting from the initial seed\n",
    "    return start_string + ' ' + ' '.join(text_generated)\n",
    "\n",
    "# Generate text with temperature control\n",
    "start_string = \"To be, or not to be\"\n",
    "generated_text = generate_text(model, start_string, temperature=0.7) # Lower temperature for more focused predictions\n",
    "print(generated_text)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30c0218",
   "metadata": {},
   "source": [
    "As shown, terrible output. This probably just has to do with my hardware and data limits."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
